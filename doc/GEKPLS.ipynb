{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "",
  "signature": "sha256:d78bd013d2a8c9bff3ac3b21cb52fe7b4bddd4c8a9c7865a467f2e90b2879375"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# GEKPLS\n",
      "######[Nomenclature](./nomenclature.html)\n",
      "\n",
      "\n",
      "GEKPLS is a gradient-enhaced kriging with partial least squares approach.\n",
      "Gradient-enhaced kriging (GEK) is an extention of kriging which supports gradient information [1].\n",
      "GEK is usually more accurate than kriging, however, it is not computationally efficient when the number of inputs, the number of sampling points, or both, are high.\n",
      "This is mainly due to the size of the corresponding correlation matrix that increases proportionally with both the number of inputs and the number of sampling points.\n",
      "\n",
      "To adress these issues, GEKPLS exploits the gradient information with a slight increase of the size of the correlation matrix and reduces the number of hyperparameters.\n",
      "The key idea of GEKPLS consists in generating a set of approximating points around each sampling points using the first order Taylor approximation method.\n",
      "Then, the PLS method is applied several times, each time on a different number of sampling points with the associated sampling points.\n",
      "Each PLS provides a set of coefficients that gives the contribution of each variable nearby the associated sampling point to the output.\n",
      "Finally, an average of all PLS coefficients is computed to get the global influence to the output.\n",
      "Denoting these coefficients by $\\left(w_1^{(k)},\\dots,w_d^{(k)}\\right)$, the GEKPLS Gaussian kernel function is given by:\n",
      "\n",
      "\\begin{equation}\n",
      "k\\left({\\bf x^{(i)}},{\\bf x^{(j)}}\\right)=\\sigma\\prod\\limits_{l=1}^d \\prod\\limits_{k=1}^h\\exp\\left(-\\theta_k\\left(w_l^{(k)}x_l^{(i)}-w_l^{(k)}x_l^{(j)}\\right)^{2}\\right)\n",
      "\\end{equation}\n",
      "This approach reduces the number of hyperparameters (reduced dimension) from $d$ to $h$ with $d>>h$.\n",
      "\n",
      "As previously mentioned, PLS is applied several times with respect to each sampling point, which provides the influence of each input variable around that point.\n",
      "The idea here is to add only m approximating points $(m \\in [1, d])$ around each sampling point. \n",
      "Only the $m$ highest coefficients given by the first principal component are considered, which usually contains the most useful information. \n",
      "More details of such approach are given in [2].\n",
      "\n",
      "\n",
      "[1] I. J. Forrester, A. Sobester, and A. J. Keane, Engineering Design via Surrogate Modeling: A Practical Guide. Wiley, 2008. (Chapter 7)\n",
      "\n",
      "[2] Bouhlel, M. A. and Martins, J. R. R. A., Gradient-enhanced kriging for high-dimensional problems (under review), Engineering with\n",
      "Computers, 2017.\n",
      "\n",
      "\n",
      "## Options\n",
      "\n",
      "| Option | Default | Acceptable values | Acceptable types | Description |\n",
      "| - | - | - | - | - |\n",
      "| n_comp | 1 | None | ['int'] | Number of principal components |\n",
      "| delta_x | 0.0001 | None | ['int', 'float'] | Step used in the FOTA |\n",
      "| print_solver | True | None | ['bool'] | Whether to print solver information |\n",
      "| print_problem | True | None | ['bool'] | Whether to print problem information |\n",
      "| print_global | True | None | ['bool'] | Global print toggle. If False, all printing is suppressed |\n",
      "| extra_points | 0 | None | ['int'] | Number of extra points per training point |\n",
      "| theta0 | [0.01] | None | ['list', 'ndarray'] | Initial hyperparameters |\n",
      "| poly | constant | ('constant', 'linear', 'quadratic') | ['function'] | regr. term |\n",
      "| corr | squar_exp | ('abs_exp', 'squar_exp') | ['function'] | type of corr. func. |\n",
      "| data_dir | None | None | ['str'] | Directory for loading / saving cached data; None means do not save or load |\n",
      "| print_prediction | True | None | ['bool'] | Whether to print prediction information |\n",
      "| print_training | True | None | ['bool'] | Whether to print training information |\n",
      "| xlimits | None | None | ['ndarray'] | Lower/upper bounds in each dimension - ndarray [nx, 2] |\n",
      "\n",
      "## Example code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function, division\n",
      "import numpy as np\n",
      "from smt.methods.gekpls import GEKPLS\n",
      "from scipy import linalg\n",
      "from smt.problems import Sphere\n",
      "from smt.sampling import LHS\n",
      "\n",
      "\n",
      "# Initialization of the problem\n",
      "ndim = 10\n",
      "ndoe = int(10*ndim)\n",
      "\n",
      "# Define the function\n",
      "fun = Sphere(ndim = ndim)\n",
      "\n",
      "# Construction of the DOE\n",
      "sampling = LHS(xlimits=fun.xlimits,criterion = 'm')\n",
      "xt = sampling(ndoe)\n",
      "\n",
      "# Compute the output\n",
      "yt = fun(xt)\n",
      "# Compute the gradient\n",
      "for i in range(ndim):\n",
      "    yd = fun(xt,kx=i)\n",
      "    yt = np.concatenate((yt,yd),axis=1)\n",
      "\n",
      "# Construction of the validation points\n",
      "ntest = 500\n",
      "sampling = LHS(xlimits=fun.xlimits)\n",
      "xtest = sampling(ntest)\n",
      "ytest = fun(xtest)\n",
      "\n",
      "########### The GEKPLS model using 1 approximating points\n",
      "t = GEKPLS(n_comp=1, theta0=[1e-2], xlimits=fun.xlimits,delta_x=1e-4,extra_points= 1)\n",
      "t.add_training_points('exact',xt,yt[:,0])\n",
      "# Add the gradient information\n",
      "for i in range(ndim):\n",
      "    t.add_training_points('exact',xt,yt[:, 1+i].reshape((yt.shape[0],1)),kx=i)\n",
      "\n",
      "t.train()\n",
      "y = t.predict_value(xtest)\n",
      "\n",
      "print('GEKPLS1,  err: '+str(linalg.norm(y.reshape((ntest,1))-ytest.reshape((ntest,\n",
      "            1)))/linalg.norm(ytest.reshape((ntest,1)))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "___________________________________________________________________________\n",
        "   \n",
        "                                  GEKPLS\n",
        "___________________________________________________________________________\n",
        "   \n",
        " Problem size\n",
        "   \n",
        "      # training points.        : 100\n",
        "   \n",
        "___________________________________________________________________________\n",
        "   \n",
        " Training\n",
        "   \n",
        "   Training ...\n",
        "   Training - done. Time (sec):  0.1533151\n",
        "___________________________________________________________________________\n",
        "   \n",
        " Evaluation\n",
        "   \n",
        "      # eval points. : 500\n",
        "   \n",
        "   Predicting ...\n",
        "   Predicting - done. Time (sec):  0.0110800\n",
        "   \n",
        "   Prediction time/pt. (sec) :  0.0000222\n",
        "   \n",
        "GEKPLS1,  err: 0.00206913932703\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}